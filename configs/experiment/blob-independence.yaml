# @package _global_
defaults:
  - /data: blob
  - /model: mlp
  - /operator@tau2:
      - swap


project: "BLOB-transforms"
seed: 0

tau1: null

tau2:
  swap:
    p: 3
    d: 2

model:
  input_size: 6
  hidden_layer_size: [50, 50]

data:
  samples: 2700
  type: "type2"
  with_labels: true

train:
  lr: 0.0005
  earlystopping:
    patience: 20
    delta: 0.0
  epochs: 1000
  seqs: 1 # number of mini-batches
  T: 0 #Warm start number of mini-batches used for the training only
  alpha: 0.05 # significance level
  batch_size: 90
  l1_lambda: 0.0
  l2_lambda: 0.0


