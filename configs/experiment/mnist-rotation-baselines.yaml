# @package _global_
defaults:
  - /data: rotatedmnist
  - /operator@tau1:
      - projection
  - /operator@tau2:
      - swap
      - projection
      - reshape
      - rotation



project: "Test"
seed: 0

tau1:
  projection:
    input_dim: 0


tau2:
  swap:
    p: 784 # size of the image when flattened
    d: 0
  projection:
    input_dim: 0
  rotation:
    num_rotations: 4


data:
  samples: 100
  type: "type2"
  data_seed: 100
  p: 0.3

model:
  _target_: "models.mlp.MLP"
  input_size: 784
  hidden_layer_size: [128, 64]
  drop_out: true
  drop_out_p: 0.5
  layer_norm: false
  output_size: 2

train:
  ps: 4
  name: "rand"
  seed: 0
  lr: 0.0005
  earlystopping:
    patience: 5
    delta: 0.0
  epochs: 1000
  seqs: 10 # number of mini-batches
  T: 0 #Warm start number of mini-batches used for the training only
  alpha: 0.05 # significance level
  batch_size: 64
  save: false
  save_dir: ""
  l1_lambda: 0.01
  l2_lambda: 0.01