# @package _global_
defaults:
  - /data: blob
  - /model: mlp
  - /operator@tau1:
      - projection
  - /operator@tau2:
      - swap
      - projection

project: "BLOB-2ST-Baselines"

tau1:
  projection:
    input_dim: 0

tau2:
  swap:
    p: 2
    d: 0
  projection:
    input_dim: 0


model:
  _target_: "models.mlp.MLP"
  input_size: 2
  hidden_layer_size: [30, 30]
  output_size: 2
  bias: true
  drop_out: false
  drop_out_p: 0.0
  layer_norm: true


data:
  samples: 90
  type: "type11"
  with_labels: false
  data_seed: 0


train:
  seed: 0
  lr: 0.0005
  earlystopping:
    patience: 10
    delta: 0.0
  epochs: 500
  seqs: 1 # number of mini-batches
  T: 0 #Warm start number of mini-batches used for the training only
  alpha: 0.05 # significance level
  batch_size: 90
  save: false
  save_dir: "path/to/deep-anytime-testing/logs/blob/2st/"
  l1_lambda: 0.0
  l2_lambda: 0.0